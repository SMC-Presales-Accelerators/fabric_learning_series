{"cells":[{"cell_type":"code","source":["parquet_name = \"\"\n","parquet_path = \"\"\n","match_method = \"\""],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"tags":["parameters"]},"id":"e417b0d5-cb63-4bb9-99ba-98ed6b3444be"},{"cell_type":"markdown","source":["Above you can see our use of the parameters cell again, this time though we are sending the parameters via the runMultiple command.\n","\n","Below the cell is merging the newly read Parquet file to the currently existing delta table with the pyspark merge method. We use the provided match_method parameter to handle checking if the row already exists in our data set."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"ad4a4d2d-c0c5-4ecc-8e05-6c4eba14f628"},{"cell_type":"code","source":["from delta.tables import *\n","\n","table_name = parquet_name.split('.')[0]\n","print(\"Merging table {table_name} from {file_name}\".format(table_name=table_name, file_name=parquet_name))\n","current_table = spark.read.parquet(parquet_path)\n","\n","deltaTable = DeltaTable.forPath(spark, \"Tables/\" + table_name.lower())\n","\n","(deltaTable.alias(\"target\")\n","  .merge(current_table.alias(\"source\"), match_method)\n","  .whenMatchedUpdateAll()\n","  .whenNotMatchedInsertAll()\n","  .execute()\n",")\n"],"outputs":[],"execution_count":null,"metadata":{},"id":"d0294c3c-9095-4c7d-9151-1190ab4ca5ba"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}